{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92b012d3",
   "metadata": {},
   "source": [
    "# Understanding Political Bias in News Text Classification\n",
    "\n",
    "This notebook trains a BERT classifier on MBIC and compares baseline predictions to a masked-text variant to test reliance on explicitly biased terms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817fffb0",
   "metadata": {},
   "source": [
    "## Step 1: Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9b0d74",
   "metadata": {},
   "source": [
    "Import libraries for data handling, modeling, and explainability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e4979ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rodrigobatista/PycharmProjects/4ano/1semestre/IAS/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2026-01-03 23:50:44.776270: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2026-01-03 23:50:44.806649: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2026-01-03 23:50:45.566476: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments, AutoTokenizer\n",
    "from collections import defaultdict\n",
    "from datasets import Dataset\n",
    "from transformers_interpret import SequenceClassificationExplainer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc6edcc",
   "metadata": {},
   "source": [
    "## Step 2: Data loading and split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af567a6",
   "metadata": {},
   "source": [
    "Load the cleaned MBIC dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79cd192b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"datasets/labeled_dataset_cleaned.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8033f500",
   "metadata": {},
   "source": [
    "Split the dataset into train/test with stratification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c5bba45",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df[\"sentence\"],\n",
    "    df[\"type\"],\n",
    "    test_size=0.2,\n",
    "    stratify=df[\"type\"],\n",
    "    random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f3c3fd",
   "metadata": {},
   "source": [
    "Create train/test DataFrames aligned with the split indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69e6d314",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = df.loc[X_train.index].copy()\n",
    "test_df  = df.loc[X_test.index].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4971875",
   "metadata": {},
   "source": [
    "Define label-id mappings for the three classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "356a9a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "label2id = {\n",
    "    \"left\": 0,\n",
    "    \"center\": 1,\n",
    "    \"right\": 2\n",
    "}\n",
    "\n",
    "id2label = {v: k for k, v in label2id.items()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf78e07",
   "metadata": {},
   "source": [
    "Map string labels to numeric ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "114d67b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"label\"] = train_df[\"type\"].map(label2id)\n",
    "test_df[\"label\"]  = test_df[\"type\"].map(label2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c65d5ca",
   "metadata": {},
   "source": [
    "Convert Pandas DataFrames to Hugging Face Datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67ee24ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset.from_pandas(train_df[[\"sentence\", \"label\"]])\n",
    "test_dataset  = Dataset.from_pandas(test_df[[\"sentence\", \"label\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12638069",
   "metadata": {},
   "source": [
    "Inspect the dataset objects (optional sanity check)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c41bee1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['sentence', 'label', '__index_level_0__'],\n",
       "     num_rows: 782\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['sentence', 'label', '__index_level_0__'],\n",
       "     num_rows: 196\n",
       " }))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset, test_dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1e3d25",
   "metadata": {},
   "source": [
    "## Step 3: Tokenization and dataset preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e86fa1",
   "metadata": {},
   "source": [
    "Load the BERT tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "381f53c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31aa75c1",
   "metadata": {},
   "source": [
    "Define the tokenization function and maximum sequence length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "13870532",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 512\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(\n",
    "        batch[\"sentence\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=MAX_LEN\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd257e07",
   "metadata": {},
   "source": [
    "Tokenize the train/test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc7852d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 782/782 [00:00<00:00, 7553.74 examples/s]\n",
      "Map: 100%|██████████| 196/196 [00:00<00:00, 8012.28 examples/s]\n"
     ]
    }
   ],
   "source": [
    "train_dataset = train_dataset.map(tokenize, batched=True)\n",
    "test_dataset  = test_dataset.map(tokenize, batched=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6404e368",
   "metadata": {},
   "source": [
    "Remove text columns and set dataset format to PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "94f5ec63",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.remove_columns([\"sentence\"])\n",
    "test_dataset  = test_dataset.remove_columns([\"sentence\"])\n",
    "\n",
    "train_dataset.set_format(\"torch\")\n",
    "test_dataset.set_format(\"torch\")  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e62b0e0",
   "metadata": {},
   "source": [
    "Remove index columns added during conversion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "48bd6acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.remove_columns([\"__index_level_0__\"])\n",
    "test_dataset  = test_dataset.remove_columns([\"__index_level_0__\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cafb4cab",
   "metadata": {},
   "source": [
    "## Step 4: BERT training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33886a76",
   "metadata": {},
   "source": [
    "Initialize the BERT classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "199f4d9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_name = \"bert-base-uncased\" \n",
    "\n",
    "bert_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=3,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7c506e",
   "metadata": {},
   "source": [
    "Define evaluation metrics (macro F1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e9267b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=1)\n",
    "    return {\n",
    "        \"macro_f1\": f1_score(labels, preds, average=\"macro\")\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9096c0",
   "metadata": {},
   "source": [
    "Compute class weights to address imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5f387d31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.6896, 5.2133, 0.7363])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight=\"balanced\",\n",
    "    classes=np.array([0, 1, 2]),\n",
    "    y=train_df[\"label\"].values\n",
    ")\n",
    "\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float)\n",
    "class_weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48952dca",
   "metadata": {},
   "source": [
    "Create a custom Trainer with class-weighted loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a001a427",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class WeightedTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "\n",
    "        loss_fct = torch.nn.CrossEntropyLoss(\n",
    "            weight=class_weights.to(logits.device)\n",
    "        )\n",
    "        loss = loss_fct(logits, labels)\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca17588",
   "metadata": {},
   "source": [
    "Set training hyperparameters and evaluation strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a9843032",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./bert_bias\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=50,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"macro_f1\",\n",
    "    greater_is_better=True,\n",
    "    report_to=\"none\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2195192c",
   "metadata": {},
   "source": [
    "Train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6260f828",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='490' max='490' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [490/490 01:19, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Macro F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.116000</td>\n",
       "      <td>1.058791</td>\n",
       "      <td>0.373476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>1.118879</td>\n",
       "      <td>0.409931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.618400</td>\n",
       "      <td>1.148632</td>\n",
       "      <td>0.486962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.367900</td>\n",
       "      <td>1.240505</td>\n",
       "      <td>0.447467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.265600</td>\n",
       "      <td>1.311401</td>\n",
       "      <td>0.451178</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=490, training_loss=0.657240999961386, metrics={'train_runtime': 79.9133, 'train_samples_per_second': 48.928, 'train_steps_per_second': 6.132, 'total_flos': 1028773463316480.0, 'train_loss': 0.657240999961386, 'epoch': 5.0})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = WeightedTrainer(\n",
    "    model=bert_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38ef173",
   "metadata": {},
   "source": [
    "Evaluate the model on the test set and print metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6310bcb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        left      0.682     0.632     0.656        95\n",
      "      center      0.250     0.077     0.118        13\n",
      "       right      0.635     0.750     0.688        88\n",
      "\n",
      "    accuracy                          0.648       196\n",
      "   macro avg      0.522     0.486     0.487       196\n",
      "weighted avg      0.632     0.648     0.634       196\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preds = trainer.predict(test_dataset)\n",
    "y_true = preds.label_ids\n",
    "y_pred = np.argmax(preds.predictions, axis=1)\n",
    "\n",
    "print(classification_report(\n",
    "    y_true,\n",
    "    y_pred,\n",
    "    target_names=[id2label[i] for i in range(3)],\n",
    "    digits=3\n",
    "))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59162d24",
   "metadata": {},
   "source": [
    "Switch to evaluation mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "985adfbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_model.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24158fb",
   "metadata": {},
   "source": [
    "## Step 5: Evaluation and sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92936853",
   "metadata": {},
   "source": [
    "Convert logits to probabilities and compute confidence scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7f0a1c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    return exp_x / exp_x.sum(axis=1, keepdims=True)\n",
    "\n",
    "probs = softmax(preds.predictions)\n",
    "conf = probs.max(axis=1)\n",
    "y_pred = np.argmax(probs, axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a13bbc",
   "metadata": {},
   "source": [
    "Build a results table with predictions and confidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8fa59660",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_bert = pd.DataFrame({\n",
    "    \"text\": test_df[\"sentence\"],   \n",
    "    \"true\": [id2label[i] for i in y_true],\n",
    "    \"pred\": [id2label[i] for i in y_pred],\n",
    "    \"confidence\": conf\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69620a30",
   "metadata": {},
   "source": [
    "Helper function to sample instances by confidence and correctness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "39821668",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_instances(\n",
    "    df,\n",
    "    true_label,\n",
    "    correct=True,\n",
    "    conf_min=None,\n",
    "    conf_max=None,\n",
    "    n=10\n",
    "):\n",
    "    subset = df[df[\"true\"] == true_label]\n",
    "\n",
    "    if correct is not None:\n",
    "        if correct:\n",
    "            subset = subset[subset[\"true\"] == subset[\"pred\"]]\n",
    "        else:\n",
    "            subset = subset[subset[\"true\"] != subset[\"pred\"]]\n",
    "\n",
    "    if conf_min is not None:\n",
    "        subset = subset[subset[\"confidence\"] >= conf_min]\n",
    "\n",
    "    if conf_max is not None:\n",
    "        subset = subset[subset[\"confidence\"] <= conf_max]\n",
    "\n",
    "    return subset.sort_values(\"confidence\", ascending=False).head(n)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9662d794",
   "metadata": {},
   "source": [
    "Select high-confidence correct examples per class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "527cc61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "left_high = select_instances(results_bert, \"left\", True, 0.60, None, n=10)\n",
    "right_high = select_instances(results_bert, \"right\", True, 0.60, None, n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33defb51",
   "metadata": {},
   "source": [
    "Preview a couple of example texts and predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0d32fc9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trump has also in his years in the White House frequently praised the use of violence. He told police officers in 2017 that they shouldn’t worry about hitting suspects’ heads as they place them in patrol cars. And on a conference call with governors on Monday, he berated them for not “dominating” the streets of their cities.\n",
      "left\n",
      "left\n",
      "==============================\n",
      "Democrats blocked a coronavirus package on Sunday that would provide economic relief to businesses and Americans suffering from the impact of the coronavirus outbreak\n",
      "right\n",
      "right\n"
     ]
    }
   ],
   "source": [
    "print(left_high.iloc[0][\"text\"][:1000])\n",
    "print(left_high.iloc[0][\"true\"])\n",
    "print(left_high.iloc[0][\"pred\"])\n",
    "print(\"==============================\")\n",
    "print(right_high.iloc[0][\"text\"][:1000])\n",
    "print(right_high.iloc[0][\"true\"])\n",
    "print(right_high.iloc[0][\"pred\"]) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64301860",
   "metadata": {},
   "source": [
    "Select low-confidence correct and incorrect cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ad44f1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "low_conf_correct = results_bert[\n",
    "    (results_bert[\"true\"] == results_bert[\"pred\"]) &\n",
    "    (results_bert[\"confidence\"] >= 0.50) &\n",
    "    (results_bert[\"confidence\"] <= 0.55)\n",
    "].sort_values(\"confidence\").head(1)\n",
    "\n",
    "low_conf_wrong = results_bert[\n",
    "    (results_bert[\"true\"] != results_bert[\"pred\"]) &\n",
    "    (results_bert[\"confidence\"] >= 0.50) &\n",
    "    (results_bert[\"confidence\"] <= 0.55)\n",
    "].sort_values(\"confidence\").head(4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37115bbd",
   "metadata": {},
   "source": [
    "Select confident errors for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a77213fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "confident_error = results_bert[\n",
    "    results_bert[\"true\"] != results_bert[\"pred\"]\n",
    "].sort_values(\"confidence\", ascending=False).head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b088a61",
   "metadata": {},
   "source": [
    "Preview a low-confidence example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "562bfc21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It’s also why attempts to portray pro-lifers as racists smack of disingenuousness.\n",
      "right\n",
      "right\n",
      "0.50949603\n"
     ]
    }
   ],
   "source": [
    "print(low_conf_correct.iloc[0][\"text\"][:1000])\n",
    "print(low_conf_correct.iloc[0][\"true\"])\n",
    "print(low_conf_correct.iloc[0][\"pred\"])\n",
    "print(low_conf_correct.iloc[0][\"confidence\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa3b89b",
   "metadata": {},
   "source": [
    "Assemble a candidate set for explanation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f5a1fccf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>true</th>\n",
       "      <th>pred</th>\n",
       "      <th>confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Trump has also in his years in the White House...</td>\n",
       "      <td>left</td>\n",
       "      <td>left</td>\n",
       "      <td>0.907582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Democrats blocked a coronavirus package on Sun...</td>\n",
       "      <td>right</td>\n",
       "      <td>right</td>\n",
       "      <td>0.847086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>It’s also why attempts to portray pro-lifers a...</td>\n",
       "      <td>right</td>\n",
       "      <td>right</td>\n",
       "      <td>0.509496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>On Wednesday, democrats got a chance to grill ...</td>\n",
       "      <td>center</td>\n",
       "      <td>left</td>\n",
       "      <td>0.506826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dem Senators Say Mismanaged Student Loan Progr...</td>\n",
       "      <td>left</td>\n",
       "      <td>right</td>\n",
       "      <td>0.830945</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text    true   pred  \\\n",
       "0  Trump has also in his years in the White House...    left   left   \n",
       "1  Democrats blocked a coronavirus package on Sun...   right  right   \n",
       "2  It’s also why attempts to portray pro-lifers a...   right  right   \n",
       "3  On Wednesday, democrats got a chance to grill ...  center   left   \n",
       "4  Dem Senators Say Mismanaged Student Loan Progr...    left  right   \n",
       "\n",
       "   confidence  \n",
       "0    0.907582  \n",
       "1    0.847086  \n",
       "2    0.509496  \n",
       "3    0.506826  \n",
       "4    0.830945  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_candidates = pd.concat([\n",
    "    left_high.iloc[[0]],\n",
    "    right_high.iloc[[0]],\n",
    "    low_conf_correct,\n",
    "    low_conf_wrong.iloc[[0]],\n",
    "    confident_error.iloc[[1]]\n",
    "]).reset_index(drop=True)\n",
    "\n",
    "bert_candidates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7284200",
   "metadata": {},
   "source": [
    "## Step 6: Explainability (token attributions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5522c86e",
   "metadata": {},
   "source": [
    "Split long texts into overlapping chunks for attribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "aeec2439",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text_with_overlap(text, tokenizer, max_length=512, overlap=50):\n",
    "    tokens = tokenizer.encode(text, add_special_tokens=False)\n",
    "    \n",
    "    chunks = []\n",
    "    start = 0\n",
    "    \n",
    "    while start < len(tokens):\n",
    "        end = start + max_length\n",
    "        chunk_tokens = tokens[start:end]\n",
    "        \n",
    "        chunk_text = tokenizer.decode(chunk_tokens, skip_special_tokens=True)\n",
    "        chunks.append(chunk_text)\n",
    "        \n",
    "        start += max_length - overlap\n",
    "    \n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa9a972",
   "metadata": {},
   "source": [
    "Aggregate token attributions across chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d546c298",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_bert_instance(\n",
    "    text,\n",
    "    explainer,\n",
    "    tokenizer,\n",
    "    max_length=400,\n",
    "    overlap=50,\n",
    "    top_k=15\n",
    "):\n",
    "    chunks = chunk_text_with_overlap(\n",
    "        text,\n",
    "        tokenizer,\n",
    "        max_length=max_length,\n",
    "        overlap=overlap\n",
    "    )\n",
    "    \n",
    "    all_attributions = defaultdict(list)\n",
    "    \n",
    "    for chunk in chunks:\n",
    "        word_attributions = explainer(chunk)\n",
    "        #explainer.visualize()\n",
    "        for token, score in word_attributions:\n",
    "            all_attributions[token].append(score)\n",
    "    \n",
    "    aggregated_attributions = {\n",
    "        token: np.mean(scores)\n",
    "        for token, scores in all_attributions.items()\n",
    "    }\n",
    "    \n",
    "    top_tokens = sorted(\n",
    "        aggregated_attributions.items(),\n",
    "        key=lambda x: abs(x[1]),\n",
    "        reverse=True\n",
    "    )[:top_k]\n",
    "    \n",
    "    return top_tokens\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f87000b",
   "metadata": {},
   "source": [
    "Run token-level explanations on selected candidates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b6f91a5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "INSTANCE 1\n",
      "TRUE: left | PRED: left | CONF: 0.91\n",
      "Top tokens:\n",
      "trump           +0.752\n",
      "also            -0.222\n",
      "praised         -0.212\n",
      "2017            +0.201\n",
      "house           +0.197\n",
      "police          +0.161\n",
      "their           +0.153\n",
      "monday          -0.116\n",
      "he              +0.104\n",
      "suspects        +0.099\n",
      "use             +0.099\n",
      "violence        +0.097\n",
      "##rated         -0.083\n",
      "white           +0.082\n",
      "cars            +0.080\n",
      "======================================================================\n",
      "INSTANCE 2\n",
      "TRUE: right | PRED: right | CONF: 0.85\n",
      "Top tokens:\n",
      "democrats       +0.594\n",
      "the             +0.296\n",
      "americans       +0.280\n",
      "a               -0.234\n",
      "blocked         -0.162\n",
      "package         +0.130\n",
      "to              -0.121\n",
      "of              +0.107\n",
      "businesses      +0.105\n",
      "and             -0.094\n",
      "would           -0.089\n",
      "sunday          +0.086\n",
      "##virus         +0.058\n",
      "from            +0.057\n",
      "that            +0.048\n",
      "======================================================================\n",
      "INSTANCE 3\n",
      "TRUE: right | PRED: right | CONF: 0.51\n",
      "Top tokens:\n",
      "smack           +0.683\n",
      ".               +0.403\n",
      "##s             -0.329\n",
      "s               -0.268\n",
      "racist          -0.192\n",
      "di              -0.169\n",
      "##sing          -0.164\n",
      "why             -0.152\n",
      "##ness          -0.124\n",
      "of              -0.112\n",
      "also            -0.099\n",
      "it              +0.098\n",
      "to              -0.092\n",
      "life            +0.077\n",
      "as              -0.061\n",
      "======================================================================\n",
      "INSTANCE 4\n",
      "TRUE: center | PRED: left | CONF: 0.51\n",
      "Top tokens:\n",
      "democrats       -0.748\n",
      "trump           +0.441\n",
      "wednesday       -0.303\n",
      "donald          +0.212\n",
      "president       +0.137\n",
      "capitol         +0.108\n",
      "got             +0.107\n",
      ".               -0.099\n",
      ",               +0.093\n",
      "andrew          +0.081\n",
      "came            +0.072\n",
      "der             +0.068\n",
      "agency          +0.058\n",
      "'               -0.058\n",
      "his             +0.053\n",
      "======================================================================\n",
      "INSTANCE 5\n",
      "TRUE: left | PRED: right | CONF: 0.83\n",
      "Top tokens:\n",
      "dem             +0.583\n",
      "thousands       +0.551\n",
      "student         +0.381\n",
      "senators        +0.379\n",
      "screwed         +0.212\n",
      "say             -0.116\n",
      "##ged           +0.085\n",
      "program         +0.042\n",
      "mis             -0.021\n",
      "##mana          +0.017\n",
      "loan            -0.012\n",
      "[CLS]           +0.000\n",
      "[SEP]           +0.000\n"
     ]
    }
   ],
   "source": [
    "explainer = SequenceClassificationExplainer(\n",
    "    bert_model,\n",
    "    tokenizer\n",
    ")\n",
    "\n",
    "for i in range(len(bert_candidates)):\n",
    "    row = bert_candidates.iloc[i]\n",
    "    \n",
    "    text = row[\"text\"]\n",
    "    true_label = row[\"true\"]\n",
    "    pred_label = row[\"pred\"]\n",
    "    conf = row[\"confidence\"]\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(f\"INSTANCE {i+1}\")\n",
    "    print(f\"TRUE: {true_label} | PRED: {pred_label} | CONF: {conf:.2f}\")\n",
    "    \n",
    "    top_tokens = explain_bert_instance(\n",
    "        text,\n",
    "        explainer,\n",
    "        tokenizer\n",
    "    )\n",
    "    \n",
    "    print(\"Top tokens:\")\n",
    "    for tok, score in top_tokens:\n",
    "        print(f\"{tok:15s} {score:+.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca76e3bb",
   "metadata": {},
   "source": [
    "## Step 7: Masked BERT pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de7b20e",
   "metadata": {},
   "source": [
    "Load the masked MBIC dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5be33651",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_masked =pd.read_excel(\"datasets/labeled_dataset_masked.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e690b2",
   "metadata": {},
   "source": [
    "Prepare masked datasets using the same train/test split and tokenize them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c991e61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 782/782 [00:00<00:00, 8559.92 examples/s]\n",
      "Map: 100%|██████████| 196/196 [00:00<00:00, 8328.61 examples/s]\n"
     ]
    }
   ],
   "source": [
    "text_col_masked = \"sentence_masked\"\n",
    "\n",
    "train_df_masked = df_masked.loc[X_train.index].copy()\n",
    "test_df_masked  = df_masked.loc[X_test.index].copy()\n",
    "\n",
    "train_df_masked[\"label\"] = train_df_masked[\"type\"].map(label2id)\n",
    "test_df_masked[\"label\"]  = test_df_masked[\"type\"].map(label2id)\n",
    "\n",
    "train_df_masked[\"sentence\"] = train_df_masked[text_col_masked]\n",
    "test_df_masked[\"sentence\"] = test_df_masked[text_col_masked]\n",
    "\n",
    "train_dataset_masked = Dataset.from_pandas(train_df_masked[[\"sentence\", \"label\"]])\n",
    "test_dataset_masked  = Dataset.from_pandas(test_df_masked[[\"sentence\", \"label\"]])\n",
    "\n",
    "train_dataset_masked = train_dataset_masked.map(tokenize, batched=True)\n",
    "test_dataset_masked  = test_dataset_masked.map(tokenize, batched=True)\n",
    "\n",
    "train_dataset_masked = train_dataset_masked.remove_columns([\"sentence\"])\n",
    "test_dataset_masked  = test_dataset_masked.remove_columns([\"sentence\"])\n",
    "\n",
    "train_dataset_masked = train_dataset_masked.remove_columns([\"__index_level_0__\"])\n",
    "test_dataset_masked  = test_dataset_masked.remove_columns([\"__index_level_0__\"])\n",
    "\n",
    "train_dataset_masked.set_format(\"torch\")\n",
    "test_dataset_masked.set_format(\"torch\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e5a71e",
   "metadata": {},
   "source": [
    "Fine-tune a new BERT model on masked text and evaluate performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "584d5551",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='490' max='490' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [490/490 01:19, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Macro F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.129200</td>\n",
       "      <td>1.073252</td>\n",
       "      <td>0.365878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.049200</td>\n",
       "      <td>1.001859</td>\n",
       "      <td>0.365544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.840500</td>\n",
       "      <td>0.950182</td>\n",
       "      <td>0.437341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.594300</td>\n",
       "      <td>1.087277</td>\n",
       "      <td>0.418412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.377000</td>\n",
       "      <td>1.183393</td>\n",
       "      <td>0.379664</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        left      0.557     0.463     0.506        95\n",
      "      center      0.172     0.385     0.238        13\n",
      "       right      0.568     0.568     0.568        88\n",
      "\n",
      "    accuracy                          0.505       196\n",
      "   macro avg      0.433     0.472     0.437       196\n",
      "weighted avg      0.536     0.505     0.516       196\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bert_model_masked = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=3,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n",
    "\n",
    "class_weights_masked = compute_class_weight(\n",
    "    class_weight=\"balanced\",\n",
    "    classes=np.array([0, 1, 2]),\n",
    "    y=train_df_masked[\"label\"].values\n",
    ")\n",
    "class_weights_masked = torch.tensor(class_weights_masked, dtype=torch.float)\n",
    "\n",
    "class WeightedTrainerMasked(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "\n",
    "        loss_fct = torch.nn.CrossEntropyLoss(\n",
    "            weight=class_weights_masked.to(logits.device)\n",
    "        )\n",
    "        loss = loss_fct(logits, labels)\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "training_args_masked = TrainingArguments(\n",
    "    output_dir=\"./bert_bias_masked\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=50,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"macro_f1\",\n",
    "    greater_is_better=True,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer_masked = WeightedTrainerMasked(\n",
    "    model=bert_model_masked,\n",
    "    args=training_args_masked,\n",
    "    train_dataset=train_dataset_masked,\n",
    "    eval_dataset=test_dataset_masked,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer_masked.train()\n",
    "\n",
    "preds_masked = trainer_masked.predict(test_dataset_masked)\n",
    "y_true_masked = preds_masked.label_ids\n",
    "y_pred_masked = np.argmax(preds_masked.predictions, axis=1)\n",
    "\n",
    "print(classification_report(\n",
    "    y_true_masked,\n",
    "    y_pred_masked,\n",
    "    target_names=[id2label[i] for i in range(3)],\n",
    "    digits=3\n",
    "))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d02497",
   "metadata": {},
   "source": [
    "Compute masked results and select candidates for explanation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "352b537b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model_masked.eval()\n",
    "\n",
    "probs_masked = softmax(preds_masked.predictions)\n",
    "conf_masked = probs_masked.max(axis=1)\n",
    "y_pred_masked = np.argmax(probs_masked, axis=1)\n",
    "\n",
    "results_bert_masked = pd.DataFrame({\n",
    "    \"text\": test_df_masked[\"sentence\"],\n",
    "    \"true\": [id2label[i] for i in y_true_masked],\n",
    "    \"pred\": [id2label[i] for i in y_pred_masked],\n",
    "    \"confidence\": conf_masked\n",
    "})\n",
    "\n",
    "left_high_m = select_instances(results_bert_masked, \"left\", True, 0.60, None, n=10)\n",
    "right_high_m = select_instances(results_bert_masked, \"right\", True, 0.60, None, n=10)\n",
    "\n",
    "low_conf_correct_m = results_bert_masked[(results_bert_masked[\"true\"] == results_bert_masked[\"pred\"]) &\n",
    "    (results_bert_masked[\"confidence\"] >= 0.50) &\n",
    "    (results_bert_masked[\"confidence\"] <= 0.55)].sort_values(\"confidence\").head(1)\n",
    "\n",
    "low_conf_wrong_m = results_bert_masked[(results_bert_masked[\"true\"] != results_bert_masked[\"pred\"]) &\n",
    "    (results_bert_masked[\"confidence\"] >= 0.50) &\n",
    "    (results_bert_masked[\"confidence\"] <= 0.55)].sort_values(\"confidence\").head(4)\n",
    "\n",
    "confident_error_m = results_bert_masked[results_bert_masked[\"true\"] != results_bert_masked[\"pred\"]]\n",
    "confident_error_m = confident_error_m.sort_values(\"confidence\", ascending=False).head(4)\n",
    "\n",
    "bert_candidates_masked = pd.concat([\n",
    "    left_high_m.iloc[[0]],\n",
    "    right_high_m.iloc[[0]],\n",
    "    low_conf_correct_m,\n",
    "    low_conf_wrong_m.iloc[[0]],\n",
    "    confident_error_m.iloc[[1]]\n",
    "]).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43db82b8",
   "metadata": {},
   "source": [
    "## Step 8: Masked explainability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b63e15",
   "metadata": {},
   "source": [
    "Explain masked-model predictions with token attributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e78150ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "MASKED INSTANCE 1\n",
      "TRUE: left | PRED: left | CONF: 0.79\n",
      "Top tokens:\n",
      "amazon          +0.572\n",
      "ceo             -0.362\n",
      "fires           +0.275\n",
      "did             -0.240\n",
      "announcement    -0.197\n",
      "##zos           -0.161\n",
      "is              +0.155\n",
      "his             -0.152\n",
      "getting         +0.146\n",
      ".               -0.146\n",
      "making          +0.145\n",
      "company         -0.144\n",
      "we              +0.142\n",
      "$               -0.139\n",
      "know            +0.135\n",
      "======================================================================\n",
      "MASKED INSTANCE 2\n",
      "TRUE: right | PRED: right | CONF: 0.74\n",
      "Top tokens:\n",
      "how             +0.345\n",
      "explained       +0.345\n",
      "was             +0.314\n",
      "s               +0.289\n",
      "an              +0.263\n",
      "patrick         +0.220\n",
      "moore           +0.206\n",
      "president       +0.197\n",
      "original        +0.196\n",
      "former          +0.172\n",
      "news            +0.158\n",
      "vision          +0.154\n",
      "and             +0.148\n",
      "br              +0.131\n",
      "##t             +0.127\n",
      "======================================================================\n",
      "MASKED INSTANCE 3\n",
      "TRUE: right | PRED: right | CONF: 0.50\n",
      "Top tokens:\n",
      ".               -0.639\n",
      "unlike          -0.358\n",
      "pregnancy       +0.325\n",
      "and             -0.221\n",
      "abortion        +0.155\n",
      "life            +0.147\n",
      "supports        +0.145\n",
      "chose           +0.123\n",
      "least           -0.119\n",
      "##s             -0.104\n",
      "son             +0.099\n",
      "north           -0.096\n",
      "an              +0.091\n",
      "children        +0.085\n",
      "his             +0.084\n",
      "======================================================================\n",
      "MASKED INSTANCE 4\n",
      "TRUE: right | PRED: left | CONF: 0.51\n",
      "Top tokens:\n",
      "tuesday         +0.432\n",
      "we              +0.286\n",
      "should          +0.269\n",
      "trump           -0.242\n",
      "convince        +0.135\n",
      "off             -0.122\n",
      "hold            -0.108\n",
      "and             -0.098\n",
      "elections       -0.086\n",
      "voters          +0.085\n",
      "media           +0.082\n",
      "\"               -0.071\n",
      "but             -0.071\n",
      "narratives      +0.070\n",
      "years           -0.056\n",
      "======================================================================\n",
      "MASKED INSTANCE 5\n",
      "TRUE: left | PRED: right | CONF: 0.75\n",
      "Top tokens:\n",
      "he              +0.471\n",
      ".               +0.444\n",
      "nancy           +0.406\n",
      "interview       +0.308\n",
      "was             +0.282\n",
      "##on            +0.238\n",
      "for             +0.142\n",
      "an              +0.131\n",
      "s               +0.128\n",
      "than            -0.124\n",
      "##eo            -0.118\n",
      "am              -0.084\n",
      "##sm            -0.078\n",
      "’               -0.066\n",
      "po              +0.044\n"
     ]
    }
   ],
   "source": [
    "explainer_masked = SequenceClassificationExplainer(\n",
    "    bert_model_masked,\n",
    "    tokenizer\n",
    ")\n",
    "\n",
    "for i in range(len(bert_candidates_masked)):\n",
    "    row = bert_candidates_masked.iloc[i]\n",
    "    text = row[\"text\"]\n",
    "    true_label = row[\"true\"]\n",
    "    pred_label = row[\"pred\"]\n",
    "    conf = row[\"confidence\"]\n",
    "\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"MASKED INSTANCE {i+1}\")\n",
    "    print(f\"TRUE: {true_label} | PRED: {pred_label} | CONF: {conf:.2f}\")\n",
    "\n",
    "    top_tokens = explain_bert_instance(\n",
    "        text,\n",
    "        explainer_masked,\n",
    "        tokenizer\n",
    "    )\n",
    "\n",
    "    print(\"Top tokens:\")\n",
    "    for tok, score in top_tokens:\n",
    "        print(f\"{tok:15s} {score:+.3f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

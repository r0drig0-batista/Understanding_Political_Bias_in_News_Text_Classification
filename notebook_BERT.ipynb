{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09b9c28f",
   "metadata": {},
   "source": [
    "# Understanding Political Bias in News Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192248d5",
   "metadata": {},
   "source": [
    "In this notebook we train a BERT classifier on the cleaned Political Bias dataset and inspect its predictions with token-level attributions. We also test a masking strategy to see whether structural phrases influence the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4b7403",
   "metadata": {},
   "source": [
    "## Step 1: Setup\n",
    "\n",
    "Import the libraries needed for data handling, modeling, and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f548570",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rodrigobatista/PycharmProjects/4ano/1semestre/IAS/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2026-01-05 00:48:07.269248: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2026-01-05 00:48:07.749205: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2026-01-05 00:48:08.883637: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from transformers_interpret import SequenceClassificationExplainer\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoModelForSequenceClassification,TrainingArguments, Trainer, AutoTokenizer\n",
    "from datasets import Dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257b4916",
   "metadata": {},
   "source": [
    "## Step 2: Data preparation\n",
    "\n",
    "Load the cleaned dataset and confirm the text and label columns are ready for modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7c2b97",
   "metadata": {},
   "source": [
    "Load the cleaned dataset exported from the preprocessing notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ea62c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"datasets/Political_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008937b6",
   "metadata": {},
   "source": [
    "Define features (text) and target labels for the 3-class setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc0da84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[\"Text\"]\n",
    "y = df[\"Bias_3\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dbd5e09",
   "metadata": {},
   "source": [
    "## Step 3: Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55106cf2",
   "metadata": {},
   "source": [
    "Create train/test splits with stratification to keep class balance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6fd0b290",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=13, stratify=y\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe179d4d",
   "metadata": {},
   "source": [
    "Build train/test DataFrames used by Hugging Face Datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69e6d314",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = df.loc[X_train.index][[\"Text\", \"Bias_3\"]].copy()\n",
    "test_df  = df.loc[X_test.index][[\"Text\", \"Bias_3\"]].copy()\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49668f3",
   "metadata": {},
   "source": [
    "Define label-id mappings used by the model and trainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "356a9a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "label2id = {\n",
    "    \"left\": 0,\n",
    "    \"center\": 1,\n",
    "    \"right\": 2\n",
    "}\n",
    "\n",
    "id2label = {v: k for k, v in label2id.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "114d67b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"label\"] = train_df[\"Bias_3\"].map(label2id)\n",
    "test_df[\"label\"]  = test_df[\"Bias_3\"].map(label2id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037e48b2",
   "metadata": {},
   "source": [
    "Convert string labels into numeric ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef0d5b6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[\"label\"].isna().sum(), test_df[\"label\"].isna().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ce2314",
   "metadata": {},
   "source": [
    "Convert Pandas DataFrames to Hugging Face Dataset objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67ee24ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset.from_pandas(train_df[[\"Text\", \"label\"]])\n",
    "test_dataset  = Dataset.from_pandas(test_df[[\"Text\", \"label\"]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c41bee1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['Text', 'label', '__index_level_0__'],\n",
       "     num_rows: 5116\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['Text', 'label', '__index_level_0__'],\n",
       "     num_rows: 1280\n",
       " }))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset, test_dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4593b897",
   "metadata": {},
   "source": [
    "Load the BERT tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "381f53c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4965c8c9",
   "metadata": {},
   "source": [
    "Define a tokenization function with truncation and padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "13870532",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 512\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(\n",
    "        batch[\"Text\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=MAX_LEN\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1234b88a",
   "metadata": {},
   "source": [
    "Tokenize the datasets and set PyTorch format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cc7852d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 5116/5116 [00:01<00:00, 4164.42 examples/s]\n",
      "Map: 100%|██████████| 1280/1280 [00:00<00:00, 5484.08 examples/s]\n"
     ]
    }
   ],
   "source": [
    "train_dataset = train_dataset.map(tokenize, batched=True)\n",
    "test_dataset  = test_dataset.map(tokenize, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "94f5ec63",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.remove_columns([\"Text\"])\n",
    "test_dataset  = test_dataset.remove_columns([\"Text\"])\n",
    "\n",
    "train_dataset.set_format(\"torch\")\n",
    "test_dataset.set_format(\"torch\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "48bd6acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.remove_columns([\"__index_level_0__\"])\n",
    "test_dataset  = test_dataset.remove_columns([\"__index_level_0__\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38980e14",
   "metadata": {},
   "source": [
    "### BERT fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "199f4d9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"bert-base-uncased\"\n",
    "\n",
    "bert_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=3,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d377ab49",
   "metadata": {},
   "source": [
    "Define evaluation metrics (macro F1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e9267b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=1)\n",
    "    return {\n",
    "        \"macro_f1\": f1_score(labels, preds, average=\"macro\")\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5f387d31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4998, 3.5977, 1.3864])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weights = compute_class_weight(\n",
    "    class_weight=\"balanced\",\n",
    "    classes=np.array([0, 1, 2]),\n",
    "    y=train_df[\"label\"].values\n",
    ")\n",
    "\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float)\n",
    "class_weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a4f378",
   "metadata": {},
   "source": [
    "Custom Trainer that applies class-weighted loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a001a427",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "\n",
    "        loss_fct = torch.nn.CrossEntropyLoss(\n",
    "            weight=class_weights.to(logits.device)\n",
    "        )\n",
    "        loss = loss_fct(logits, labels)\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a24eb0",
   "metadata": {},
   "source": [
    "Set training hyperparameters and evaluation strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a9843032",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./bert_bias\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=50,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"macro_f1\",\n",
    "    greater_is_better=True,\n",
    "    report_to=\"none\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19429b04",
   "metadata": {},
   "source": [
    "Train the BERT model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6260f828",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3200' max='3200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3200/3200 08:03, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Macro F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.463800</td>\n",
       "      <td>0.494037</td>\n",
       "      <td>0.789393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.468400</td>\n",
       "      <td>0.537777</td>\n",
       "      <td>0.834903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.167800</td>\n",
       "      <td>0.569954</td>\n",
       "      <td>0.845153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.091100</td>\n",
       "      <td>0.791647</td>\n",
       "      <td>0.858719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.065200</td>\n",
       "      <td>0.769442</td>\n",
       "      <td>0.854929</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3200, training_loss=0.3080393965728581, metrics={'train_runtime': 484.0183, 'train_samples_per_second': 52.849, 'train_steps_per_second': 6.611, 'total_flos': 6730441225482240.0, 'train_loss': 0.3080393965728581, 'epoch': 5.0})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = WeightedTrainer(\n",
    "    model=bert_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483c4f08",
   "metadata": {},
   "source": [
    "Evaluate on the test set and print the classification report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6310bcb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        left      0.928     0.948     0.938       854\n",
      "      center      0.785     0.712     0.747       118\n",
      "       right      0.903     0.880     0.891       308\n",
      "\n",
      "    accuracy                          0.910      1280\n",
      "   macro avg      0.872     0.847     0.859      1280\n",
      "weighted avg      0.909     0.910     0.909      1280\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preds = trainer.predict(test_dataset)\n",
    "y_true = preds.label_ids \n",
    "y_pred = np.argmax(preds.predictions, axis=1)\n",
    "\n",
    "print(classification_report(\n",
    "    y_true,\n",
    "    y_pred,\n",
    "    target_names=[id2label[i] for i in range(3)],\n",
    "    digits=3\n",
    "))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "985adfbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_model.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d82248",
   "metadata": {},
   "source": [
    "Compute probabilities and confidence scores from model logits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ff1921fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    return exp_x / exp_x.sum(axis=1, keepdims=True)\n",
    "\n",
    "probs = softmax(preds.predictions)\n",
    "conf = probs.max(axis=1)\n",
    "y_pred = np.argmax(probs, axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d8690c",
   "metadata": {},
   "source": [
    "Aggregate predictions and confidence into a results table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c740e7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_bert = pd.DataFrame({\n",
    "    \"text\": test_df[\"Text\"],   \n",
    "    \"true\": [id2label[i] for i in y_true],\n",
    "    \"pred\": [id2label[i] for i in y_pred],\n",
    "    \"confidence\": conf\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc1ed1c",
   "metadata": {},
   "source": [
    "Helper to sample examples by correctness and confidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "af4011e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_instances(\n",
    "    df,\n",
    "    true_label,\n",
    "    correct=True,\n",
    "    conf_min=None,\n",
    "    conf_max=None,\n",
    "    n=10\n",
    "):\n",
    "    subset = df[df[\"true\"] == true_label]\n",
    "\n",
    "    if correct is not None:\n",
    "        if correct:\n",
    "            subset = subset[subset[\"true\"] == subset[\"pred\"]]\n",
    "        else:\n",
    "            subset = subset[subset[\"true\"] != subset[\"pred\"]]\n",
    "\n",
    "    if conf_min is not None:\n",
    "        subset = subset[subset[\"confidence\"] >= conf_min]\n",
    "\n",
    "    if conf_max is not None:\n",
    "        subset = subset[subset[\"confidence\"] <= conf_max]\n",
    "\n",
    "    return subset.sort_values(\"confidence\", ascending=False).head(n)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b5ac81",
   "metadata": {},
   "source": [
    "Pick high-confidence correct examples per class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ea0f9873",
   "metadata": {},
   "outputs": [],
   "source": [
    "left_high = select_instances(results_bert, \"left\", True, 0.75, None, n=10)\n",
    "right_high = select_instances(results_bert, \"right\", True, 0.75, None, n=10)\n",
    "center_high = select_instances(results_bert, \"center\", True, 0.75, None, n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a05cc9",
   "metadata": {},
   "source": [
    "Quickly inspect example texts and predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "981a4796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cardinal Dr. Robert Prevost has officially taken the name Pope Leo XIV, becoming the first American head of the Catholic Church in history. And a tweet Prevost wrote earlier this year about Vice President JD Vance is going viral. In February, Prevost amplified an op-ed published in the National Catholic Reporter in which author Kat Armas criticized the vice president (who converted to Catholicism in 2019) over his remarks suggesting there was a hierarchy of Christian priorities. Vance told Fox News in late January: \"There is a Christian concept that you love your family and then you love your neighbor, and then you love your community, and then you love your fellow citizens, and then after that, prioritize the rest of the world. A lot of the far left has completely inverted that.\" Prevost's tweet repeated the headline of the op-ed: \"JD Vance is wrong: Jesus doesn't ask us to rank our love for others.\" READ MORE: (Opinion) Jasmine Crockett shames Republicans  again \"[The Apostle] Paul r\n",
      "left\n",
      "left\n",
      "==============================\n",
      "A Phoenix, Arizona, woman confronted and shot an alleged intruder inside her home Wednesday around 1:40 p.m. FOX 10 reported the woman entered her home and found an alleged female intruder inside. The woman shot the intruder and 12 News pointed out the alleged home intruder was taken to a nearby hospital to be treated for her injuries. The injuries were not believed to be life-threatening. Breitbart News noted that a Rabbi in Baltimore, Maryland, scared away alleged carjackers by pulling a gun on them over the Memorial Weekend. By subscribing, you agree to our terms of use & privacy policy. You will receive email marketing messages from Breitbart News Network to the email you provide. You may unsubscribe at any time. The masked suspects allegedly assaulted the Rabbi once and were beginning to do so again when he produced the gun and sent them fleeing. AWR Hawkins is an award-winning Second Amendment columnist for Breitbart News and the writer/curator of Down Range with AWR Hawkins, a w\n",
      "right\n",
      "right\n"
     ]
    }
   ],
   "source": [
    "print(left_high.iloc[0][\"text\"][:1000])\n",
    "print(left_high.iloc[0][\"true\"])\n",
    "print(left_high.iloc[0][\"pred\"])\n",
    "print(\"==============================\")\n",
    "print(right_high.iloc[0][\"text\"][:1000])\n",
    "print(right_high.iloc[0][\"true\"])\n",
    "print(right_high.iloc[0][\"pred\"]) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2db10f4",
   "metadata": {},
   "source": [
    "Select low-confidence correct and incorrect cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "88aa51ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "low_conf_correct = results_bert[\n",
    "    (results_bert[\"true\"] == results_bert[\"pred\"]) &\n",
    "    (results_bert[\"confidence\"] >= 0.50) &\n",
    "    (results_bert[\"confidence\"] <= 0.55)\n",
    "].sort_values(\"confidence\").head(1)\n",
    "\n",
    "low_conf_wrong = results_bert[\n",
    "    (results_bert[\"true\"] != results_bert[\"pred\"]) &\n",
    "    (results_bert[\"confidence\"] >= 0.50) &\n",
    "    (results_bert[\"confidence\"] <= 0.55)\n",
    "].sort_values(\"confidence\").head(4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b52cb5",
   "metadata": {},
   "source": [
    "Select confident errors for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9564443a",
   "metadata": {},
   "outputs": [],
   "source": [
    "confident_error = results_bert[\n",
    "    results_bert[\"true\"] != results_bert[\"pred\"]\n",
    "].sort_values(\"confidence\", ascending=False).head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "42dce6ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "President Donald Trump has never kept his contempt for the Federal Emergency Management Agency a secret, contending that the agency has been operating poorly and rarely helped disaster victims. On Friday, while touring North Carolina neighborhoods that were ravaged by Hurricane Helene, the president said he was planning an executive order that would \"begin the process of fundamentally reforming and overhauling FEMA, or maybe getting rid of them.\" His order would create a task force that would look for reforms, according to sources. However, Trump's authority does not give him the power to terminate the agency unilaterally, according to federal laws. Doing so would require congressional action. FEMA is part of the Department of Homeland Security, which has an operating budget and disaster relief fund that needs to be replenished by Congress every year to help states deal with disaster recovery, preparedness, response and mitigation efforts. The U.S. government uses the funds to reimburs\n",
      "left\n",
      "left\n",
      "0.5048652\n"
     ]
    }
   ],
   "source": [
    "print(low_conf_correct.iloc[0][\"text\"][:1000])\n",
    "print(low_conf_correct.iloc[0][\"true\"])\n",
    "print(low_conf_correct.iloc[0][\"pred\"])\n",
    "print(low_conf_correct.iloc[0][\"confidence\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec274f1",
   "metadata": {},
   "source": [
    "Assemble a balanced set of candidates for explanation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "de7a8673",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>true</th>\n",
       "      <th>pred</th>\n",
       "      <th>confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cardinal Dr. Robert Prevost has officially tak...</td>\n",
       "      <td>left</td>\n",
       "      <td>left</td>\n",
       "      <td>0.999684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Less than one week after Donald Trump's inaugu...</td>\n",
       "      <td>left</td>\n",
       "      <td>left</td>\n",
       "      <td>0.999678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A Phoenix, Arizona, woman confronted and shot ...</td>\n",
       "      <td>right</td>\n",
       "      <td>right</td>\n",
       "      <td>0.999269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Biden administration allegedly discriminat...</td>\n",
       "      <td>right</td>\n",
       "      <td>right</td>\n",
       "      <td>0.999245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>President Donald Trump campaigned on a promise...</td>\n",
       "      <td>center</td>\n",
       "      <td>center</td>\n",
       "      <td>0.999611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>President Donald Trump has never kept his cont...</td>\n",
       "      <td>left</td>\n",
       "      <td>left</td>\n",
       "      <td>0.504865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Musk steps down from government efficiency role</td>\n",
       "      <td>center</td>\n",
       "      <td>left</td>\n",
       "      <td>0.519047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>President Donald Trump on Friday fired at leas...</td>\n",
       "      <td>center</td>\n",
       "      <td>left</td>\n",
       "      <td>0.999586</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text    true    pred  \\\n",
       "0  Cardinal Dr. Robert Prevost has officially tak...    left    left   \n",
       "1  Less than one week after Donald Trump's inaugu...    left    left   \n",
       "2  A Phoenix, Arizona, woman confronted and shot ...   right   right   \n",
       "3  The Biden administration allegedly discriminat...   right   right   \n",
       "4  President Donald Trump campaigned on a promise...  center  center   \n",
       "5  President Donald Trump has never kept his cont...    left    left   \n",
       "6    Musk steps down from government efficiency role  center    left   \n",
       "7  President Donald Trump on Friday fired at leas...  center    left   \n",
       "\n",
       "   confidence  \n",
       "0    0.999684  \n",
       "1    0.999678  \n",
       "2    0.999269  \n",
       "3    0.999245  \n",
       "4    0.999611  \n",
       "5    0.504865  \n",
       "6    0.519047  \n",
       "7    0.999586  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_candidates = pd.concat([\n",
    "    left_high.iloc[[0]],\n",
    "    left_high.iloc[[3]],\n",
    "    right_high.iloc[[0]],\n",
    "    right_high.iloc[[1]],\n",
    "    center_high.iloc[[0]],\n",
    "    low_conf_correct,\n",
    "    low_conf_wrong.iloc[[0]],\n",
    "    confident_error.iloc[[1]]\n",
    "]).reset_index(drop=True)\n",
    "\n",
    "bert_candidates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8035493b",
   "metadata": {},
   "source": [
    "## Step 4: Explainability (BERT)\n",
    "\n",
    "We approximate token importance using transformers-interpret. Because texts can be long, we split them into overlapping chunks and aggregate token attributions across chunks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8f3d8a",
   "metadata": {},
   "source": [
    "Split long texts into overlapping chunks for attribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ef65fd03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text_with_overlap(text, tokenizer, max_length=512, overlap=50):\n",
    "    tokens = tokenizer.encode(text, add_special_tokens=False)\n",
    "    \n",
    "    chunks = []\n",
    "    start = 0\n",
    "    \n",
    "    while start < len(tokens):\n",
    "        end = start + max_length\n",
    "        chunk_tokens = tokens[start:end]\n",
    "        \n",
    "        chunk_text = tokenizer.decode(chunk_tokens, skip_special_tokens=True)\n",
    "        chunks.append(chunk_text)\n",
    "        \n",
    "        start += max_length - overlap\n",
    "    \n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b122bb58",
   "metadata": {},
   "source": [
    "Aggregate token attributions across chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f33f418d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def explain_bert_instance(\n",
    "    text,\n",
    "    explainer,\n",
    "    tokenizer,\n",
    "    max_length=400,\n",
    "    overlap=50,\n",
    "    top_k=15\n",
    "):\n",
    "    chunks = chunk_text_with_overlap(\n",
    "        text,\n",
    "        tokenizer,\n",
    "        max_length=max_length,\n",
    "        overlap=overlap\n",
    "    )\n",
    "    \n",
    "    all_attributions = defaultdict(list)\n",
    "    \n",
    "    for chunk in chunks:\n",
    "        word_attributions = explainer(chunk)\n",
    "        #explainer.visualize()\n",
    "        for token, score in word_attributions:\n",
    "            all_attributions[token].append(score)\n",
    "    \n",
    "    aggregated_attributions = {\n",
    "        token: np.mean(scores)\n",
    "        for token, scores in all_attributions.items()\n",
    "    }\n",
    "    \n",
    "    top_tokens = sorted(\n",
    "        aggregated_attributions.items(),\n",
    "        key=lambda x: abs(x[1]),\n",
    "        reverse=True\n",
    "    )[:top_k]\n",
    "    \n",
    "    return top_tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf765417",
   "metadata": {},
   "source": [
    "Generate token attributions for each selected example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a475f685",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (685 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "INSTANCE 1\n",
      "TRUE: left | PRED: left | CONF: 1.00\n",
      "Top tokens:\n",
      "robert          +0.182\n",
      "head            -0.168\n",
      "author          +0.104\n",
      "family          +0.101\n",
      "earlier         -0.097\n",
      "d               -0.095\n",
      "february        +0.078\n",
      "news            -0.077\n",
      "history         +0.076\n",
      "national        +0.074\n",
      "he              -0.073\n",
      "uk              +0.072\n",
      "reporter        +0.067\n",
      "2019            +0.062\n",
      "stop            -0.057\n",
      "======================================================================\n",
      "INSTANCE 2\n",
      "TRUE: left | PRED: left | CONF: 1.00\n",
      "Top tokens:\n",
      "one             +0.176\n",
      "building        -0.104\n",
      "of              +0.104\n",
      "week            +0.100\n",
      "joseph          +0.097\n",
      "\"               +0.089\n",
      "than            -0.076\n",
      "president       +0.075\n",
      "signed          +0.060\n",
      "127             +0.050\n",
      "donald          +0.047\n",
      "has             +0.046\n",
      "office          +0.043\n",
      "martin          +0.041\n",
      "rhode           +0.041\n",
      "======================================================================\n",
      "INSTANCE 3\n",
      "TRUE: right | PRED: right | CONF: 1.00\n",
      "Top tokens:\n",
      "shot            +0.237\n",
      "##eit           +0.140\n",
      "br              +0.125\n",
      "inside          +0.109\n",
      "female          +0.061\n",
      "winning         +0.060\n",
      "award           +0.054\n",
      "##bar           +0.054\n",
      "found           +0.051\n",
      "phoenix         -0.051\n",
      "you             +0.050\n",
      "##t             +0.043\n",
      "news            +0.043\n",
      "confronted      -0.038\n",
      "woman           -0.038\n",
      "======================================================================\n",
      "INSTANCE 4\n",
      "TRUE: right | PRED: right | CONF: 1.00\n",
      "Top tokens:\n",
      "##eit           +0.243\n",
      "elizabeth       +0.225\n",
      "vargas          +0.199\n",
      "reported        +0.190\n",
      "amazon          +0.172\n",
      "memorandum      +0.147\n",
      "br              +0.146\n",
      "##pol           -0.118\n",
      "sunday          +0.117\n",
      "is              +0.113\n",
      "robert          -0.109\n",
      "fellowship      -0.098\n",
      "charge          +0.097\n",
      "##bar           +0.094\n",
      "lessons         -0.090\n",
      "======================================================================\n",
      "INSTANCE 5\n",
      "TRUE: center | PRED: center | CONF: 1.00\n",
      "Top tokens:\n",
      "download        +0.911\n",
      "stories         -0.286\n",
      "social          +0.267\n",
      "learn           +0.188\n",
      "im              +0.102\n",
      "related         +0.093\n",
      "facts           +0.087\n",
      "email           +0.083\n",
      "camera          +0.082\n",
      "today           +0.078\n",
      "##cr            -0.074\n",
      "my              -0.069\n",
      "##ibe           -0.063\n",
      "anytime         -0.063\n",
      "states          -0.060\n",
      "======================================================================\n",
      "INSTANCE 6\n",
      "TRUE: left | PRED: left | CONF: 0.50\n",
      "Top tokens:\n",
      "kept            -0.167\n",
      "told            +0.159\n",
      "rules           +0.148\n",
      "touring         -0.139\n",
      "abc             +0.138\n",
      "both            -0.126\n",
      "donald          +0.114\n",
      "secret          -0.114\n",
      "late            -0.107\n",
      "contempt        +0.104\n",
      "report          -0.084\n",
      "president       -0.084\n",
      "last            -0.080\n",
      "majority        -0.079\n",
      "contend         -0.076\n",
      "======================================================================\n",
      "INSTANCE 7\n",
      "TRUE: center | PRED: left | CONF: 0.52\n",
      "Top tokens:\n",
      "efficiency      -0.660\n",
      "role            -0.423\n",
      "mu              -0.362\n",
      "government      -0.302\n",
      "##sk            -0.249\n",
      "steps           -0.249\n",
      "down            -0.201\n",
      "from            -0.022\n",
      "[CLS]           +0.000\n",
      "[SEP]           +0.000\n",
      "======================================================================\n",
      "INSTANCE 8\n",
      "TRUE: center | PRED: left | CONF: 1.00\n",
      "Top tokens:\n",
      "whom            +0.331\n",
      "more            +0.254\n",
      ":               -0.219\n",
      "statement       +0.214\n",
      "rep             +0.211\n",
      "sen             +0.207\n",
      "connolly        +0.139\n",
      "its             +0.129\n",
      "d               +0.111\n",
      "nbc             +0.102\n",
      "gerry           +0.090\n",
      "writing         -0.089\n",
      "x               -0.084\n",
      "sent            -0.080\n",
      "we              -0.079\n"
     ]
    }
   ],
   "source": [
    "\n",
    "explainer = SequenceClassificationExplainer(\n",
    "    bert_model,\n",
    "    tokenizer\n",
    ")\n",
    "\n",
    "for i in range(len(bert_candidates)):\n",
    "    row = bert_candidates.iloc[i]\n",
    "    \n",
    "    text = row[\"text\"]\n",
    "    true_label = row[\"true\"]\n",
    "    pred_label = row[\"pred\"]\n",
    "    conf = row[\"confidence\"]\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(f\"INSTANCE {i+1}\")\n",
    "    print(f\"TRUE: {true_label} | PRED: {pred_label} | CONF: {conf:.2f}\")\n",
    "    \n",
    "    top_tokens = explain_bert_instance(\n",
    "        text,\n",
    "        explainer,\n",
    "        tokenizer\n",
    "    )\n",
    "    \n",
    "    print(\"Top tokens:\")\n",
    "    for tok, score in top_tokens:\n",
    "        print(f\"{tok:15s} {score:+.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a4962c",
   "metadata": {},
   "source": [
    "## Step 5: Masking structural phrases\n",
    "\n",
    "To mimic the earlier LIME-based analysis, we mask common structural phrases (e.g., 'READ MORE') and compare predicted probabilities. If confidence drops sharply after masking, the model may be relying on structure rather than content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9b1b3677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Cardinal Dr. Robert Prevost has officially taken the name Pope Leo XIV, becoming the first American head of the Catholic Church in history. And a tweet Prevost wrote earlier this year about Vice President JD Vance is going viral. In February, Prevost amplified an op-ed published in the National Catholic Reporter in which author Kat Armas criticized the vice president (who converted to Catholicism in 2019) over his remarks suggesting there was a hierarchy of Christian priorities. Vance told Fox News in late January: \"There is a Christian concept that you love your family and then you love your neighbor, and then you love your community, and then you love your fellow citizens, and then after that, prioritize the rest of the world. A lot of the far left has completely inverted that.\" Prevost's tweet repeated the headline of the op-ed: \"JD Vance is wrong: Jesus doesn't ask us to rank our love for others.\" [UNK] (Opinion) Jasmine Crockett shames Republicans  again \"[The Apostle] Paul reminds them: love starts close. It moves first toward those in front of us, ensuring widows were not abandoned while preserving the church's resources for those truly without support,\" Armas wrote in the op-ed. \"But make no mistake  this isn't about love confined to bloodlines or geographic boundaries. It's about love rooted in responsibility, expanding outward. And it was subversive even then.\" Prevost/Leo XIV's tweet lit up social media, with various journalists and commentators like Democratic strategist Matt McDermott celebrating \"our new woke pope.\" \"Well this will be fun,\" Independent D.C. bureau chief Eric Michael Garcia tweeted. \"Get in loser we're combing through the new pope's old tweets,\" Business Insider senior politics reporter Bryan Metzger tweeted. [UNK] 'Incoherence': Trump slammed after reporter accuses him to his face of 'overstating' UK trade deal Dan Cluchey, who was a speechwriter for former President Joe Biden, also celebrated the tweet by observing that the \"new pope already upholding the only tradition that matters: s----ing on JD Vance.\" \"STOP STOP IF I START LIKING THE CHICAGO POPE ANY MORE I'LL FORGET THE REFORMATION,\" tweeted theologian Dr. Laura Robinson. Tahra Hoops, who is the director of economic analysis at the Progress Chamber, combed into the op-ed Leo XIV tweeted and opined that he was \"abundance-pilled,\" referring to the political theory that public policy should be oriented around making sure all members of society have a high standard of living. President Donald Trump has not yet commented on Leo XIV's tweet, but delivered a statement on his Truth Social account writing: \"Congratulations to Cardinal Robert Francis Prevost, who was just named Pope. It is such an honor to realize that he is the first American Pope. What excitement, and what a Great Honor for our Country. I look forward to meeting Pope Leo XIV. It will be a very meaningful moment!\" [UNK] 'Complete fraud': Dems bring the receipts after MAGA convert gets exposed as a 'grifter'\n",
      "Example: left\n",
      "Original: {'left': '1.000', 'center': '0.000', 'right': '0.000'}\n",
      "Masked:   {'left': '1.000', 'center': '0.000', 'right': '0.000'}\n",
      "======================================================================\n",
      "A Phoenix, Arizona, woman confronted and shot an alleged intruder inside her home Wednesday around 1:40 p.m. FOX 10 reported the woman entered her home and found an alleged female intruder inside. The woman shot the intruder and 12 News pointed out the alleged home intruder was taken to a nearby hospital to be treated for her injuries. The injuries were not believed to be life-threatening. [UNK] noted that a Rabbi in Baltimore, Maryland, scared away alleged carjackers by pulling a gun on them over the Memorial Weekend. By subscribing, you agree to our terms of use & privacy policy. You will receive email marketing messages from [UNK] Network to the email you provide. You may unsubscribe at any time. The masked suspects allegedly assaulted the Rabbi once and were beginning to do so again when he produced the gun and sent them fleeing. AWR Hawkins is an award-winning Second Amendment columnist for [UNK] and the writer/curator of Down Range with AWR Hawkins, a weekly newsletter focused on all things Second Amendment, also for [UNK]. He is the political analyst for Armed American Radio, a member of Gun Owners of America, and the director of global marketing for Lone Star Hunts. He was a Visiting Fellow at the Russell Kirk Center for Cultural Renewal in 2010 and has a Ph.D. in Military History. Follow him on Instagram: @awr_hawkins. You can sign up to get Down Range at breitbart.com/downrange. Reach him directly at awrhawkins@breitbart.com.\n",
      "Example: right\n",
      "Original: {'left': '0.001', 'center': '0.000', 'right': '0.999'}\n",
      "Masked:   {'left': '0.001', 'center': '0.000', 'right': '0.999'}\n",
      "======================================================================\n",
      "President Donald Trump campaigned on a promise to end the war in Ukraine in 24 hours once in office. However, new developments and a series of back-and-forth drone attacks between Russia and Ukraine serve as signs fighting isnt slowing down. Ukraines military launched drones into Russia Friday, Jan. 24, hitting oil facilities in Ryazan and a microelectronics production plant in Bryansk. The attacks are the latest in Ukraines effort to disrupt the Russian military and put pressure on the countrys economy by striking the oil industry, which is Moscows main source of revenue. [UNK] Russian drones also attacked Ukraine on Friday, killing three people in separate locations near Kyiv, while also damaging multiple buildings.  Ukrainian President Volodymyr Zelenskyy posted a video to X showing the rescue efforts with a message saying he was grateful to every leader and every country exerting pressure on Russia to bring this unprovoked and terrorist war to an end. All night long, rescue operations continued in the Kyiv region following a Shahed drone attack. Residential buildings were damaged in Brovary and Hlevakha. Tragically, three people lost their lives. My condolences to their families and loved ones.\n",
      "\n",
      "There are also wounded who are pic.twitter.com/4cluCF9B2B The attacks come as President Trump says Zelenskyy shares the blame for the start of the war. You know hes no angel, Trump told Fox News about Zelenskyy. He shouldnt have allowed this war to happen. Trump also mentioned Russian President Vladimir Putin this week in a post on Truth Social. Im going to do Russia, whose economy is failing, and President Putin, a very big favor. Settle now, and stop this ridiculous war! Its only going to get worse, Trump said. [UNK] Trump said he is ready to meet with Putin as soon as possible, giving an ultimatum that if the war doesnt end he will put higher taxes, tariffs and sanctions on anything being sold by Russia to the United States. [UNK]\n",
      "Example: center\n",
      "Original: {'left': '0.000', 'center': '1.000', 'right': '0.000'}\n",
      "Masked:   {'left': '0.002', 'center': '0.000', 'right': '0.998'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def bert_predict_proba(texts, model, tokenizer, max_length=512):\n",
    "    device = next(model.parameters()).device\n",
    "    enc = tokenizer(\n",
    "        texts,\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    enc = {k: v.to(device) for k, v in enc.items()}\n",
    "    with torch.no_grad():\n",
    "        logits = model(**enc).logits\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "    return probs.cpu().numpy()\n",
    "\n",
    "def mask_structure(text):\n",
    "    for phrase in [\"READ MORE:\", \"Read More\", \"read more\", \"Olivia Rondeau is a politics reporter for Breitbart News based in Washington, DC. Find her on X/Twitter and Instagram.\", \"Breitbart News\", \"RELATED STORIES\", \"Learn more about our emails. Unsubscribe anytime. By entering your email, you agree to the Terms & Conditions and acknowledge the Privacy Policy.\", \"Download the SAN app today to stay up-to-date with Unbiased. Straight Facts. Point phone camera here\"]:\n",
    "        text = text.replace(phrase, \"[UNK]\")\n",
    "    return text\n",
    "\n",
    "class_names = [id2label[i] for i in range(3)]\n",
    "examples = {\n",
    "    \"left\": left_high.iloc[0][\"text\"],\n",
    "    \"right\": right_high.iloc[0][\"text\"],\n",
    "    \"center\": center_high.iloc[0][\"text\"],\n",
    "}\n",
    "\n",
    "for label, text in examples.items():\n",
    "    masked = mask_structure(text)\n",
    "    probs = bert_predict_proba([text, masked], bert_model, tokenizer)\n",
    "    print(\"=\" * 70)\n",
    "    print(masked)\n",
    "    print(f\"Example: {label}\")\n",
    "    print(\"Original:\", {cls: f\"{p:.3f}\" for cls, p in zip(class_names, probs[0])})\n",
    "    print(\"Masked:  \", {cls: f\"{p:.3f}\" for cls, p in zip(class_names, probs[1])})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3f5696",
   "metadata": {},
   "source": [
    "### Token attributions on masked text\n",
    "\n",
    "We can also run the interpreter on the masked version to see which tokens are most influential after removing structural phrases. This helps compare how the model shifts its attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4be87d79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Example: left\n",
      "Top tokens (original):\n",
      "robert          +0.182\n",
      "head            -0.168\n",
      "author          +0.104\n",
      "family          +0.101\n",
      "earlier         -0.097\n",
      "d               -0.095\n",
      "february        +0.078\n",
      "news            -0.077\n",
      "history         +0.076\n",
      "national        +0.074\n",
      "he              -0.073\n",
      "uk              +0.072\n",
      "reporter        +0.067\n",
      "2019            +0.062\n",
      "stop            -0.057\n",
      "Top tokens (masked):\n",
      "jasmine         +0.250\n",
      "opinion         +0.147\n",
      "uk              +0.122\n",
      "him             +0.118\n",
      "(               +0.100\n",
      "republicans     -0.097\n",
      "if              +0.087\n",
      "trade           -0.080\n",
      "speech          -0.078\n",
      "by              +0.076\n",
      "business        -0.073\n",
      "michael         +0.072\n",
      "left            +0.070\n",
      "##uche          +0.069\n",
      "c               +0.068\n",
      "======================================================================\n",
      "Example: right\n",
      "Top tokens (original):\n",
      "shot            +0.237\n",
      "##eit           +0.140\n",
      "br              +0.125\n",
      "inside          +0.109\n",
      "female          +0.061\n",
      "winning         +0.060\n",
      "award           +0.054\n",
      "##bar           +0.054\n",
      "found           +0.051\n",
      "phoenix         -0.051\n",
      "you             +0.050\n",
      "##t             +0.043\n",
      "news            +0.043\n",
      "confronted      -0.038\n",
      "woman           -0.038\n",
      "Top tokens (masked):\n",
      "##eit           +0.301\n",
      "shot            +0.234\n",
      "br              +0.214\n",
      "##bar           +0.110\n",
      "inside          +0.097\n",
      "woman           -0.086\n",
      "you             +0.065\n",
      "entered         +0.046\n",
      "over            -0.039\n",
      "noted           +0.039\n",
      "maryland        -0.034\n",
      "wednesday       -0.032\n",
      "a               -0.032\n",
      "found           +0.028\n",
      "an              +0.028\n",
      "======================================================================\n",
      "Example: center\n",
      "Top tokens (original):\n",
      "download        +0.911\n",
      "stories         -0.286\n",
      "social          +0.267\n",
      "learn           +0.188\n",
      "im              +0.102\n",
      "related         +0.093\n",
      "facts           +0.087\n",
      "email           +0.083\n",
      "camera          +0.082\n",
      "today           +0.078\n",
      "##cr            -0.074\n",
      "my              -0.069\n",
      "##ibe           -0.063\n",
      "anytime         -0.063\n",
      "states          -0.060\n",
      "Top tokens (masked):\n",
      "however         +0.237\n",
      "jan             -0.237\n",
      "told            +0.183\n",
      "states          -0.149\n",
      "now             -0.138\n",
      "fox             +0.121\n",
      "its             -0.103\n",
      "news            +0.100\n",
      "com             +0.098\n",
      "sanctions       -0.094\n",
      "!               +0.091\n",
      "hitting         +0.091\n",
      "campaigned      +0.088\n",
      "office          +0.085\n",
      "president       +0.076\n"
     ]
    }
   ],
   "source": [
    "def explain_with_mask(text, explainer, tokenizer, max_length=400, overlap=50, top_k=15):\n",
    "    masked = mask_structure(text)\n",
    "    orig_top = explain_bert_instance(text, explainer, tokenizer, max_length=max_length, overlap=overlap, top_k=top_k)\n",
    "    masked_top = explain_bert_instance(masked, explainer, tokenizer, max_length=max_length, overlap=overlap, top_k=top_k)\n",
    "    return masked, orig_top, masked_top\n",
    "\n",
    "examples = {\n",
    "    \"left\": left_high.iloc[0][\"text\"],\n",
    "    \"right\": right_high.iloc[0][\"text\"],\n",
    "    \"center\": center_high.iloc[0][\"text\"],\n",
    "}\n",
    "\n",
    "for label, text in examples.items():\n",
    "    masked_text, orig_top, masked_top = explain_with_mask(text, explainer, tokenizer)\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Example: {label}\")\n",
    "    print(\"Top tokens (original):\")\n",
    "    for tok, score in orig_top:\n",
    "        print(f\"{tok:15s} {score:+.3f}\")\n",
    "    print(\"Top tokens (masked):\")\n",
    "    for tok, score in masked_top:\n",
    "        print(f\"{tok:15s} {score:+.3f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
